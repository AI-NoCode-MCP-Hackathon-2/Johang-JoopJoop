import os
import pdfplumber
import textwrap
import numpy as np
import faiss
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI

# =========================
# 0. ê¸°ë³¸ ì„¸íŒ…
# =========================
load_dotenv()
client = OpenAI()

EMBED_MODEL = "text-embedding-3-small"   # ì„ë² ë”©ìš©
CHAT_MODEL = "gpt-4.1-mini"              # ë‹µë³€ìš© (ì›í•˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ ì¨ë„ ë¨)

# í˜•ë‹˜ì´ ê°€ì§„ pdf ê²½ë¡œë¡œ ë°”ê¿”ì¤˜
PDF_FILES = [
    r"C:\Users\User\Desktop\RAGë§Œë“¤ê¸°\ê·¼ë¡œë²• ìš”ì•½.pdf",
    r"C:\Users\User\Desktop\RAGë§Œë“¤ê¸°\í‘œì¤€ ê·¼ë¡œê³„ì•½ì„œ.pdf",
    r"C:\Users\User\Desktop\RAGë§Œë“¤ê¸°\ê·¼ë¡œê¸°ì¤€ë²•.pdf"
]

# =========================
# 1. PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# =========================
def extract_text_from_pdf(path: str) -> str:
    """pdfplumberë¡œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    texts = []
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            t = page.extract_text() or ""
            texts.append(t)
    full_text = "\n".join(texts)
    return full_text

# =========================
# 2. í…ìŠ¤íŠ¸ ì²­í¬ í•¨ìˆ˜
# =========================
def chunk_text(text: str,
               max_chars: int = 800,
               overlap: int = 200) -> list[dict]:
    """
    ë‹¨ìˆœ char ê¸°ì¤€ ì²­í¬.
    - max_chars: ì²­í¬ ìµœëŒ€ ê¸¸ì´
    - overlap : ì• ì²­í¬ì™€ ê²¹ì¹˜ëŠ” ë¶€ë¶„(ë¬¸ë§¥ ìœ ì§€ìš©)
    """
    cleaned = " ".join(text.split())  # ì¤„ë°”ê¿ˆ/ê³µë°± ì •ë¦¬
    chunks = []
    start = 0
    idx = 0

    while start < len(cleaned):
        end = start + max_chars
        chunk = cleaned[start:end]

        # ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ëŠê¸°ë©´ ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
        last_dot = chunk.rfind(".")
        if last_dot != -1 and last_dot > max_chars * 0.4:
            end = start + last_dot + 1
            chunk = cleaned[start:end]

        chunks.append({
            "chunk_id": idx,
            "text": chunk
        })
        idx += 1
        start = end - overlap  # overlap ë§Œí¼ ê²¹ì¹˜ê²Œ
        if start < 0:
            start = 0

    return chunks

# =========================
# 3. ëª¨ë“  PDF â†’ ì²­í¬ + DataFrame
# =========================
def build_chunks_from_pdfs(pdf_paths: list[str]) -> pd.DataFrame:
    rows = []
    doc_id = 0

    for path in pdf_paths:
        doc_id += 1
        raw_text = extract_text_from_pdf(path)
        chunks = chunk_text(raw_text)

        for c in chunks:
            rows.append({
                "doc_id": doc_id,
                "source": os.path.basename(path),
                "chunk_id": c["chunk_id"],
                "text": c["text"]
            })

    df = pd.DataFrame(rows)
    return df

# =========================
# 4. ì„ë² ë”© + FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
# =========================
def embed_texts(texts: list[str]) -> np.ndarray:
    """
    OpenAI ì„ë² ë”© API í˜¸ì¶œ
    """
    # í•œ ë²ˆì— ì—¬ëŸ¬ ê°œ ì…ë ¥ ê°€ëŠ¥
    resp = client.embeddings.create(
        model=EMBED_MODEL,
        input=texts
    )
    vectors = [d.embedding for d in resp.data]
    return np.array(vectors, dtype="float32")

def build_faiss_index(df: pd.DataFrame):
    """
    df["text"] ì „ë¶€ ì„ë² ë”©í•´ì„œ FAISS ì¸ë±ìŠ¤ ìƒì„±
    """
    embeddings = embed_texts(df["text"].tolist())
    dim = embeddings.shape[1]

    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    return index, embeddings

# =========================
# 5. ì§ˆì˜ ì‹œ: ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰ + GPT í˜¸ì¶œ
# =========================
def search_similar_chunks(query: str,
                          df: pd.DataFrame,
                          index,
                          top_k: int = 5):
    q_vec = embed_texts([query])
    distances, indices = index.search(q_vec, top_k)
    idxs = indices[0]

    results = df.iloc[idxs].copy()
    results["score"] = distances[0]
    return results

def answer_question(query: str,
                    df: pd.DataFrame,
                    index,
                    top_k: int = 5) -> str:
    # 1) ê´€ë ¨ ì²­í¬ ì°¾ê¸°
    hits = search_similar_chunks(query, df, index, top_k=top_k)

    # 2) context ë¬¸ìì—´ ë§Œë“¤ê¸°
    context_parts = []
    for _, row in hits.iterrows():
        header = f"[{row['source']} / chunk {row['chunk_id']}]"
        context_parts.append(header + "\n" + row["text"])
    context = "\n\n".join(context_parts)

    # 3) GPTì— ì§ˆì˜
    system_prompt = (
        "ë„ˆëŠ” í•œêµ­ ê·¼ë¡œê³„ì•½ì„œì™€ ê·¼ë¡œê¸°ì¤€ë²• ì•ˆë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ "
        "ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë²•ë¥  ì„¤ëª… ë„ìš°ë¯¸ì•¼. "
        "ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ì‚¼ì•„ì„œ, "
        "ì‰¬ìš´ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜. "
        "í™•ì‹¤í•˜ì§€ ì•Šì€ ë¶€ë¶„ì€ ëª¨ë¥¸ë‹¤ê³  ë§í•´."
    )

    user_prompt = f"""
ë‹¤ìŒì€ ì°¸ê³ í•´ì•¼ í•  ë¬¸ì„œ ì¼ë¶€ì•¼:

{context}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.

ì§ˆë¬¸: {query}
"""

    chat_resp = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.1,
    )

    return chat_resp.choices[0].message.content.strip()

# =========================
# 6. ì‹¤í–‰ ì˜ˆì‹œ
# =========================
if __name__ == "__main__":
    print("ğŸ“š PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ & ì²­í¬ ìƒì„± ì¤‘...")
    df_chunks = build_chunks_from_pdfs(PDF_FILES)
    print(f"ì´ ì²­í¬ ê°œìˆ˜: {len(df_chunks)}")

    print("ğŸ§  ì„ë² ë”© ìƒì„± & FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    index, _ = build_faiss_index(df_chunks)
    print("ì™„ë£Œ!")

    # ìƒ˜í”Œ ì§ˆì˜
    while True:
        q = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit'): ").strip()
        if q.lower() in ("quit", "exit"):
            break

        answer = answer_question(q, df_chunks, index, top_k=5)
        print("\n=== ë‹µë³€ ===")
        print(textwrap.fill(answer, width=80))
