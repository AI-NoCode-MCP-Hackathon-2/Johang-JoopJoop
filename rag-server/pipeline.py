import os
import pdfplumber
import numpy as np
import faiss
import pandas as pd
import re
from dotenv import load_dotenv
from openai import OpenAI

# =========================
# 0. ê¸°ë³¸ ì„¸íŒ…
# =========================
load_dotenv()
client = OpenAI()

EMBED_MODEL = "text-embedding-3-small"   # ì„ë² ë”©ìš©
CHAT_MODEL = "gpt-4.1-mini"              # ë‹µë³€ìš© (ì›í•˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ ì¨ë„ ë¨)

# PDF íŒŒì¼ ê²½ë¡œ (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
import pathlib
_BASE_DIR = pathlib.Path(__file__).parent
PDF_FILES = [
    str(_BASE_DIR / "í‘œì¤€ ê·¼ë¡œê³„ì•½ì„œ.pdf"),
    str(_BASE_DIR / "ê·¼ë¡œê¸°ì¤€ë²•.pdf"),
    str(_BASE_DIR / "ê·¼ë¡œë²• ìš”ì•½-ë³µì‚¬_OCR.pdf"),
]

# =========================
# 1. PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# =========================
def extract_text_from_pdf(path: str) -> str:
    """pdfplumberë¡œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    texts = []
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            t = page.extract_text() or ""
            texts.append(t)
    full_text = "\n".join(texts)
    return full_text

# =========================
# 2. í…ìŠ¤íŠ¸ ì²­í¬ í•¨ìˆ˜ (ê°œì„ ë¨)
# =========================

def chunk_text_ocr(text: str, source_name: str = "") -> list[dict]:
    """
    OCR ë¬¸ì„œìš© ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (ê·¼ë¡œë²• ìš”ì•½-ë³µì‚¬_OCR)
    - OCR íŠ¹ìˆ˜ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬: â€¢, o, â– , Q, c
    - ì œnì¡° íŒ¨í„´ìœ¼ë¡œ ì¡°í•­ ë¶„ë¦¬
    """
    _ = source_name
    chunks = []
    chunk_id = 0

    # ì£¼ìš” êµ¬ë¶„ì: â€¢, â– , o, O, Q, â—‹ ë˜ëŠ” ì œnì¡°
    major_split_pattern = r'(?=[â€¢â– oOQâ—‹â—]|\nì œ\d+ì¡°)'
    sections = re.split(major_split_pattern, text)

    current_header = ""
    current_content = []

    for section in sections:
        section = section.strip()
        if not section or len(section) < 20:
            continue

        # ì œnì¡° íŒ¨í„´ ê°ì§€
        article_match = re.search(r'ì œ(\d+)ì¡°', section[:100])
        if article_match:
            # ì´ì „ ì²­í¬ ì €ì¥
            if current_content:
                full_text = current_header + "\n" + "\n".join(current_content)
                if len(full_text.strip()) > 50:
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": full_text.strip(),
                        "article": article_match.group(0) if article_match else "unknown"
                    })
                    chunk_id += 1
                current_content = []

            current_header = f"ã€{article_match.group(0)}ã€‘"
            current_content.append(section)

        # OCR íŠ¹ìˆ˜ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” í•­ëª©
        elif re.match(r'^[â€¢â– oOQâ—‹â—]', section):
            # ë„ˆë¬´ ê¸¸ë©´ ìƒˆ ì²­í¬ë¡œ
            if len("\n".join(current_content)) > 1500:
                if current_content:
                    full_text = current_header + "\n" + "\n".join(current_content)
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": full_text.strip(),
                        "article": "unknown"
                    })
                    chunk_id += 1
                current_content = [section]
            else:
                current_content.append(section)
        else:
            current_content.append(section)

    # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
    if current_content:
        full_text = current_header + "\n" + "\n".join(current_content)
        if len(full_text.strip()) > 50:
            chunks.append({
                "chunk_id": chunk_id,
                "text": full_text.strip(),
                "article": "unknown"
            })

    return chunks if chunks else [{"chunk_id": 0, "text": text[:2000], "article": "unknown"}]

def chunk_text_smart(text: str, source_name: str = "") -> list[dict]:
    """
    ë²•ë¥  ë¬¸ì„œìš© ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
    - ì¡°í•­ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì œXXì¡°)
    - ê° ì¡°í•­ì´ ë„ˆë¬´ ê¸¸ë©´ í•­ ë‹¨ìœ„ë¡œ ì¶”ê°€ ë¶„ë¦¬
    - ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì œëª©ê³¼ ì¡°í•­ë²ˆí˜¸ í¬í•¨
    """
    _ = source_name  # í–¥í›„ í™•ì¥ìš©
    chunks = []

    # ì¡°í•­ íŒ¨í„´: ì œ1ì¡°, ì œ23ì¡° ë“±
    article_pattern = r'(ì œ\d+ì¡°(?:ì˜\d+)?)\(([^)]+)\)'

    # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¡°í•­ìœ¼ë¡œ ë¶„ë¦¬
    articles = re.split(r'(?=ì œ\d+ì¡°)', text)

    chunk_id = 0
    for article_text in articles:
        if not article_text.strip():
            continue

        # ì¡°í•­ ë²ˆí˜¸ì™€ ì œëª© ì¶”ì¶œ
        match = re.search(article_pattern, article_text[:200])  # ì•ë¶€ë¶„ë§Œ ê²€ìƒ‰

        if match:
            article_num = match.group(1)
            article_title = match.group(2)
            header = f"ã€{article_num}({article_title})ã€‘\n"
        else:
            header = ""

        # ì¡°í•­ì´ ë„ˆë¬´ ê¸¸ë©´ í•­(â‘ â‘¡â‘¢) ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        if len(article_text) > 1500:
            # í•­ íŒ¨í„´: â‘ , â‘¡, â‘¢ ë“±
            paragraphs = re.split(r'(?=[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©])', article_text)

            for para in paragraphs:
                if len(para.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ ê±´ ìŠ¤í‚µ
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": header + para.strip(),
                        "article": match.group(1) if match else "unknown"
                    })
                    chunk_id += 1
        else:
            # ì¡°í•­ì´ ì ë‹¹í•œ ê¸¸ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            chunks.append({
                "chunk_id": chunk_id,
                "text": header + article_text.strip(),
                "article": match.group(1) if match else "unknown"
            })
            chunk_id += 1

    return chunks

def chunk_text(text: str,
               max_chars: int = 800,
               overlap: int = 200) -> list[dict]:
    """
    í´ë°±ìš© ê¸°ë³¸ ì²­í‚¹ (ë²•ë¥  ë¬¸ì„œê°€ ì•„ë‹Œ ê²½ìš°)
    """
    cleaned = " ".join(text.split())
    chunks = []
    start = 0
    idx = 0

    while start < len(cleaned):
        end = start + max_chars
        chunk = cleaned[start:end]

        last_dot = chunk.rfind(".")
        if last_dot != -1 and last_dot > max_chars * 0.4:
            end = start + last_dot + 1
            chunk = cleaned[start:end]

        chunks.append({
            "chunk_id": idx,
            "text": chunk
        })
        idx += 1
        start = end - overlap
        if start < 0:
            start = 0

    return chunks

# =========================
# 3. ëª¨ë“  PDF â†’ ì²­í¬ + DataFrame (ê°œì„ ë¨)
# =========================
def build_chunks_from_pdfs(pdf_paths: list[str]) -> pd.DataFrame:
    rows = []
    doc_id = 0

    for path in pdf_paths:
        doc_id += 1
        raw_text = extract_text_from_pdf(path)

        # ë²•ë¥  ë¬¸ì„œë©´ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì‚¬ìš©
        source_name = os.path.basename(path)
        if "OCR" in source_name or "ìš”ì•½" in source_name:
            # OCR ë¬¸ì„œëŠ” íŠ¹ìˆ˜ë¬¸ì ê¸°ë°˜ ì²­í‚¹
            chunks = chunk_text_ocr(raw_text, source_name)
        elif "ë²•" in source_name or "ê³„ì•½ì„œ" in source_name:
            # ì¼ë°˜ ë²•ë¥  ë¬¸ì„œëŠ” ì¡°í•­ ê¸°ë°˜ ì²­í‚¹
            chunks = chunk_text_smart(raw_text, source_name)
        else:
            # ê¸°íƒ€ëŠ” ê¸°ë³¸ ì²­í‚¹
            chunks = chunk_text(raw_text)

        for c in chunks:
            rows.append({
                "doc_id": doc_id,
                "source": source_name,
                "chunk_id": c["chunk_id"],
                "text": c["text"],
                "article": c.get("article", "unknown")
            })

    df = pd.DataFrame(rows)
    return df

# =========================
# 4. ì„ë² ë”© + FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
# =========================
def embed_texts(texts: list[str]) -> np.ndarray:
    """
    OpenAI ì„ë² ë”© API í˜¸ì¶œ
    """
    # í•œ ë²ˆì— ì—¬ëŸ¬ ê°œ ì…ë ¥ ê°€ëŠ¥
    resp = client.embeddings.create(
        model=EMBED_MODEL,
        input=texts
    )
    vectors = [d.embedding for d in resp.data]
    return np.array(vectors, dtype="float32")

def build_faiss_index(df: pd.DataFrame):
    """
    df["text"] ì „ë¶€ ì„ë² ë”©í•´ì„œ FAISS ì¸ë±ìŠ¤ ìƒì„±
    """
    embeddings = embed_texts(df["text"].tolist())
    dim = embeddings.shape[1]

    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    return index, embeddings

# =========================
# 5. ì§ˆì˜ ì‹œ: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ë¡ ì  + í‚¤ì›Œë“œ)
# =========================
def keyword_search(query: str, df: pd.DataFrame, top_k: int = 10) -> set:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ - ì¡°í•­ ë²ˆí˜¸ë‚˜ ì¤‘ìš” í‚¤ì›Œë“œ ë§¤ì¹­"""
    matched_indices = set()

    # ì¡°í•­ ë²ˆí˜¸ ê²€ìƒ‰ (ì˜ˆ: "ì œ50ì¡°")
    article_match = re.search(r'ì œ\d+ì¡°', query)
    if article_match:
        article_num = article_match.group(0)
        matches = df[df['text'].str.contains(article_num, na=False)]
        matched_indices.update(matches.index.tolist()[:top_k])

    # ì£¼ìš” í‚¤ì›Œë“œ ê²€ìƒ‰
    keywords = ['ì„ê¸ˆ', 'íœ´ê°€', 'ê·¼ë¡œì‹œê°„', 'í•´ê³ ', 'ê³„ì•½', 'íœ´ì¼', 'ìˆ˜ë‹¹']
    for keyword in keywords:
        if keyword in query:
            matches = df[df['text'].str.contains(keyword, na=False)]
            matched_indices.update(matches.head(5).index.tolist())

    return matched_indices

def search_similar_chunks(query: str,
                          df: pd.DataFrame,
                          index,
                          top_k: int = 8):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ì˜ë¯¸ë¡ ì  + í‚¤ì›Œë“œ"""
    # 1) ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
    q_vec = embed_texts([query])
    distances, indices = index.search(q_vec, top_k * 2)  # ë„‰ë„‰í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°

    # 2) í‚¤ì›Œë“œ ê²€ìƒ‰
    keyword_idxs = keyword_search(query, df, top_k=5)

    # 3) ê²°í•© (í‚¤ì›Œë“œ ë§¤ì¹­ì€ ìš°ì„ ìˆœìœ„ ë†’ì„)
    combined_idxs = list(keyword_idxs) + [idx for idx in indices[0] if idx not in keyword_idxs]
    final_idxs = combined_idxs[:top_k]

    results = df.iloc[final_idxs].copy()
    results["score"] = [distances[0][list(indices[0]).index(idx)] if idx in indices[0] else 999
                        for idx in final_idxs]

    # ê´€ë ¨ì„± í•„í„°ë§: ë„ˆë¬´ ê±°ë¦¬ê°€ ë¨¼ ê²ƒ ì œì™¸
    results = results[results["score"] < 1.5]

    return results

def answer_question(query: str,
                    df: pd.DataFrame,
                    index,
                    top_k: int = 8) -> str:
    # 1) ê´€ë ¨ ì²­í¬ ì°¾ê¸°
    hits = search_similar_chunks(query, df, index, top_k=top_k)

    if len(hits) == 0:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 2) context ë¬¸ìì—´ ë§Œë“¤ê¸° (ì¤‘ë³µ ì œê±°)
    context_parts = []
    seen_texts = set()

    for _, row in hits.iterrows():
        text = row["text"]
        # ì¤‘ë³µëœ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
        if text not in seen_texts:
            seen_texts.add(text)
            context_parts.append(text)

    context = "\n\n---\n\n".join(context_parts)

    # 3) ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ GPTì— ì§ˆì˜
    system_prompt = """ë‹¹ì‹ ì€ ê·¼ë¡œê¸°ì¤€ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì—­í• **:
- ê·¼ë¡œê¸°ì¤€ë²•, ê·¼ë¡œê³„ì•½ì„œ ë“± ë…¸ë™ë²• ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ì œê³µ
- ë²•ì¡°í•­ì„ ì¸ìš©í•  ë•ŒëŠ” ì¡°í•­ ë²ˆí˜¸ë¥¼ ëª…ì‹œ (ì˜ˆ: ê·¼ë¡œê¸°ì¤€ë²• ì œ50ì¡°)
- ê·¼ë¡œìì˜ ê¶Œë¦¬ì™€ ì˜ë¬´ë¥¼ ëª…í™•íˆ ì„¤ëª…

**ë‹µë³€ ì›ì¹™**:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê·¼ê±°í•˜ì—¬ ë‹µë³€
2. ì¡°í•­ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ëª…ì‹œ
3. ë³µì¡í•œ ë²•ë¥  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
4. í™•ì‹¤í•˜ì§€ ì•Šê±°ë‚˜ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œ
5. ë‹µë³€ì€ 3~5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
6. ì¶œì²˜ë‚˜ íŒŒì¼ëª…(ì˜ˆ: PDF íŒŒì¼ëª…)ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ ê²ƒ
7. ê°€ë…ì„±ì„ ìœ„í•´ ë¬¸ë§¥ì— ë§ê²Œ ì ì ˆíˆ ì¤„ë°”ê¿ˆì„ ì‚¬ìš©í•  ê²ƒ (ì˜ˆ: í•µì‹¬ ë‚´ìš©, ì˜ˆì‹œ, ì£¼ì˜ì‚¬í•­ ë“±ì„ êµ¬ë¶„)"""

    user_prompt = f"""ë‹¤ìŒ ê·¼ë¡œê¸°ì¤€ë²• ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

{context}

---

**ì§ˆë¬¸**: {query}

**ë‹µë³€ í˜•ì‹**:
- í•´ë‹¹ ì¡°í•­ì´ ìˆë‹¤ë©´ ì¡°í•­ ë²ˆí˜¸(ì˜ˆ: ê·¼ë¡œê¸°ì¤€ë²• ì œOOì¡°)ë¥¼ ë¨¼ì € ì–¸ê¸‰
- í•µì‹¬ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…
- í•„ìš”ì‹œ ì˜ˆì‹œ í¬í•¨
- ì¶œì²˜ë‚˜ íŒŒì¼ëª…ì€ ì–¸ê¸‰í•˜ì§€ ë§ ê²ƒ"""

    chat_resp = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.0,  # ë²•ë¥  ë‹µë³€ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”
    )

    return chat_resp.choices[0].message.content.strip()

# =========================
# 6. ì‹¤í–‰ ì˜ˆì‹œ
# =========================
if __name__ == "__main__":
    print("ğŸ“š PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ & ì²­í¬ ìƒì„± ì¤‘...")
    df_chunks = build_chunks_from_pdfs(PDF_FILES)
    print(f"ì´ ì²­í¬ ê°œìˆ˜: {len(df_chunks)}")

    print("ğŸ§  ì„ë² ë”© ìƒì„± & FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    index, _ = build_faiss_index(df_chunks)
    print("ì™„ë£Œ!")

    # ìƒ˜í”Œ ì§ˆì˜
    while True:
        q = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit'): ").strip()
        if q.lower() in ("quit", "exit"):
            break

        print("\nğŸ” ê²€ìƒ‰ ì¤‘...")
        answer = answer_question(q, df_chunks, index, top_k=8)
        print("\n=== ë‹µë³€ ===")
        print(answer)
        print("\n" + "="*50)
